# supervised_fine_tuning/data_module.py

"""
Data loading utilities tailored for SFT on pairwise preference data.

We load NPZ files generated by `Data_Split.ipynb`, tokenize
them, and expose datasets compatible with Hugging Face's Trainer.
"""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List

import numpy as np
from datasets import Dataset, DatasetDict, Features, Value
from transformers import PreTrainedTokenizerBase


@dataclass
class SFTDataModule:
    """
    Utility to load and tokenize SFT datasets.

    Attributes:
        tokenizer: Hugging Face tokenizer used to encode text.
        max_length: Max token length (including prompt + response).
    """
    tokenizer: PreTrainedTokenizerBase
    max_length: int = 1024

    def _load_npz(self, path: Path) -> Dict[str, List[str]]:
        """
        Load NPZ data and convert numpy arrays to Python lists.
        """
        if not path.exists():
            raise FileNotFoundError(f"SFT data file not found: {path}")

        data = np.load(path, allow_pickle=True)
        return {key: data[key].tolist() for key in data.files}

    def _build_dataset(self, records: Dict[str, List[str]]) -> Dataset:
        """
        Convert raw prompt/response pairs into a Hugging Face Dataset.
        """
        features = Features(
            {
                "prompt": Value("string"),
                "response": Value("string"),
            }
        )
        combined_rows = [
            {"prompt": p, "response": r}
            for p, r in zip(records["prompt"], records["response_winner"])
        ]
        return Dataset.from_list(combined_rows, features=features)

    def _tokenize(self, examples: Dict[str, List[str]]) -> Dict[str, List[int]]:
        """
        Tokenize prompt-response pairs for causal LM training.
        """
        inputs = [
            f"{prompt}\n\n{response}"
            for prompt, response in zip(examples["prompt"], examples["response"])
        ]
        model_inputs = self.tokenizer(
            inputs,
            max_length=self.max_length,
            truncation=True,
            padding="max_length",
        )
        model_inputs["labels"] = model_inputs["input_ids"].copy()
        return model_inputs

    def load_dataset_dict(
        self,
        train_path: Path,
        eval_path: Path | None = None,
    ) -> DatasetDict:
        """
        Load training (and optional evaluation) datasets and tokenize them.
        """
        train_records = self._load_npz(train_path)
        train_dataset = self._build_dataset(train_records)

        dataset_dict = {"train": train_dataset}

        if eval_path is not None:
            eval_records = self._load_npz(eval_path)
            eval_dataset = self._build_dataset(eval_records)
            dataset_dict["validation"] = eval_dataset

        dataset_dict = DatasetDict(dataset_dict)
        tokenized = dataset_dict.map(
            self._tokenize,
            batched=True,
            remove_columns=dataset_dict["train"].column_names,
        )
        return tokenized